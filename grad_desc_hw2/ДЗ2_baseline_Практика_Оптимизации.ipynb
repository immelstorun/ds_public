{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyHIGVhfa_Wf"
      },
      "source": [
        "# Стохастический градиентный и координатный спуски"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Новый раздел"
      ],
      "metadata": {
        "id": "wPEBtyO2Z-IE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gn6lluIADUKa"
      },
      "source": [
        "Для каждого задания указано количество баллов (если они оцениваются отдельно) + 1 балл за аккуратное и полное выполнение всего задания"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txCccYvha_Wv"
      },
      "source": [
        "## Загрузка и подготовка данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbyOzeZ6a_Wx"
      },
      "source": [
        "**Загрузите уже знакомый вам файл *Advertising.csv* как объект DataFrame.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1L4_xeDa_Wz",
        "outputId": "4427b911-9b14-47e2-bb0a-c50f19917ff4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>TV</th>\n",
              "      <th>radio</th>\n",
              "      <th>newspaper</th>\n",
              "      <th>sales</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>230.1</td>\n",
              "      <td>37.8</td>\n",
              "      <td>69.2</td>\n",
              "      <td>22.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>44.5</td>\n",
              "      <td>39.3</td>\n",
              "      <td>45.1</td>\n",
              "      <td>10.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>17.2</td>\n",
              "      <td>45.9</td>\n",
              "      <td>69.3</td>\n",
              "      <td>9.3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0     TV  radio  newspaper  sales\n",
              "0           1  230.1   37.8       69.2   22.1\n",
              "1           2   44.5   39.3       45.1   10.4\n",
              "2           3   17.2   45.9       69.3    9.3"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = pd.read_csv('Advertising.csv')\n",
        "data.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf4aVFndDUKf"
      },
      "source": [
        "**Проверьте, есть ли в данных пропуски и, если они есть - удалите их**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiVeFnR5DUKg",
        "outputId": "9eef57ca-5f1a-4cf7-a265-3d300176cd86"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Unnamed: 0    0\n",
              "TV            0\n",
              "radio         0\n",
              "newspaper     0\n",
              "sales         0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTkiqPr_DUKh"
      },
      "source": [
        "**Преобразуйте ваши признаки в массивы NumPy и разделите их на переменные X (предикторы) и y(целевая переменная)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9OHIRB3a_Xa"
      },
      "outputs": [],
      "source": [
        "# Предикторы\n",
        "X = np.array(data[['TV','radio','newspaper']])\n",
        "# Целевая переменная\n",
        "y = np.array(data['sales'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCvjSoHEDUKo"
      },
      "source": [
        "## Координатный спуск (3 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjNm8dATDUKq"
      },
      "source": [
        "**Добавим единичный столбец для того, чтобы у нас был свободный коэффициент в уравнении регрессии:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMgq0fmKDUKr"
      },
      "outputs": [],
      "source": [
        "#import numpy as np\n",
        "X = np.hstack([np.ones(X.shape[0]).reshape(-1, 1), X])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R008OQwcDUKt"
      },
      "source": [
        "**Нормализуем данные: обычно это необходимо для корректной работы алгоритма**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Sk7Wx-SDUKt"
      },
      "outputs": [],
      "source": [
        "X = X / np.sqrt(np.sum(np.square(X), axis=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_pHHbAdDUKu"
      },
      "source": [
        "**Реализуйте алгоритм координатного спуска:** (3 балла)\n",
        "\n",
        "Ниже приведен алгоритм:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBQ8vT5UDUKu"
      },
      "source": [
        "<a href=\"https://ibb.co/Th3BQFn\"><img src=\"https://i.ibb.co/DK2DBS6/zascas.jpg\" alt=\"zascas\" border=\"0\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ce_yM20DUKv"
      },
      "source": [
        "Примечание: 1000 итераций здесь указаны для этого задания, на самом деле их может быть намного больше, нет детерменированного значения."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3IdiHm9DUKv"
      },
      "source": [
        "Вам необходимо реализовать координатный спуск, и вывести веса в модели линейной регрессии."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vsi3d9OfDUKw",
        "outputId": "d34bf0e0-3513-42bd-8b6e-ff4761ae5c8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 41.56217205]\n",
            " [110.13144155]\n",
            " [ 73.52860638]\n",
            " [ -0.55006384]]\n"
          ]
        }
      ],
      "source": [
        "y = y.reshape(-1, 1)\n",
        "\n",
        "num_iters = 1000\n",
        "# извлекаем размеры матрицы X\n",
        "m, n = X.shape\n",
        "w = np.zeros((n,1))  # Нулевой вектор весов\n",
        "r = y - X.dot(w)          # Инициализация остатков\n",
        "\n",
        "# Координатный спуск\n",
        "for iteration in range(num_iters):\n",
        "    for j in range(n):\n",
        "        # Вычисляем прогноз без k-ого фактора\n",
        "        h = (X[:,0:j] @ w[0:j]) + (X[:,j+1:] @ w[j+1:])\n",
        "        # Обновляем остатки, удаляя вклад текущего веса w_j\n",
        "        r = y - h\n",
        "        # Обновляем новое значение k-ого коэффициента\n",
        "        w[j] = (X[:,j].T @ (y - h))\n",
        "        # Вычисляем функцию потерь\n",
        "        cost = (sum((X @ w) - y) ** 2)/(len(y))\n",
        "\n",
        "# Выводим вектор весов\n",
        "print(w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKCb5fyfXkH_"
      },
      "outputs": [],
      "source": [
        "#предлагаю для простоты дальнейших расчётов представить вектор весов так:\n",
        "w = w.T[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3jG-7UADUKx"
      },
      "source": [
        "Сравните результаты с реализацией линейной регрессии из библиотеки sklearn:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBl-1Yb5DUKy",
        "outputId": "f50b6964-eb46-4bc2-9543-eb105b0790f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 41.56217205 110.13144155  73.52860638  -0.55006384]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression(fit_intercept=False)\n",
        "model.fit(X, y)\n",
        "\n",
        "print(model.coef_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIl0AGLyDUKy"
      },
      "source": [
        "Если вы все сделали верно, они должны практически совпасть!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCGwFnPdDUKz"
      },
      "source": [
        "## Стохастический градиентный спуск (6 баллов)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u7Q2YJla_Xk"
      },
      "source": [
        "**Отмасштабируйте столбцы исходной матрицы *X* (которую мы не нормализовали еще!). Для того, чтобы это сделать, надо вычесть из каждого значения среднее и разделить на стандартное отклонение** (0.5 баллов)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cEpV_5La_Xo",
        "outputId": "694c08c0-04d2-4acf-8ef9-0df199585488"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Среднее масштабированного массива: 0\n",
            "Стандартные отклонения масштабированного массива: 1\n"
          ]
        }
      ],
      "source": [
        "# Вернём изначальную (ненормализованную матрицу)\n",
        "X = np.array(data[['TV','radio','newspaper']])\n",
        "y = np.array(data['sales'])\n",
        "\n",
        "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "\n",
        "print('Среднее масштабированного массива: %.0f'%(abs(X.mean())))\n",
        "print('Стандартные отклонения масштабированного массива: %.0f'%(X.std()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WkNYILHDUK1"
      },
      "source": [
        "**Добавим единичный столбец**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVl5tEGtDUK1"
      },
      "outputs": [],
      "source": [
        "X = np.hstack([np.ones(X.shape[0]).reshape(-1, 1), X])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m53tZA5fDUK1"
      },
      "source": [
        "**Создайте функцию mse_error для вычисления среднеквадратичной ошибки, принимающую два аргумента: реальные значения и предсказывающие, и возвращающую значение mse** (0.5 балла)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cvtC08Aa_YK"
      },
      "outputs": [],
      "source": [
        "def mse_error(y, y_pred):\n",
        "    \"\"\"Функция вычисления среднеквадратичной ошибки\n",
        "\n",
        "    Args:\n",
        "        y (float): реальное значение\n",
        "        y_pred (float): предсказанное значение\n",
        "    Returns:\n",
        "        result (float): значение MSE\n",
        "    \"\"\"\n",
        "    mse = ((y - y_pred) ** 2).mean()\n",
        "    return mse # Вычисляем среднее значение реальных продаж\n",
        "\n",
        "\n",
        "mean_sales = np.mean(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpOLhdvBDUK2"
      },
      "source": [
        "**Сделайте наивный прогноз: предскажите продажи средним значением. После этого рассчитайте среднеквадратичную ошибку для этого прогноза** (0.5 балла)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLV_XljVa_YZ",
        "outputId": "b6fbf32c-e290-41b4-e052-dd41dd16d977"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Наивный прогноз (среднее значение продаж): 14.0225\n",
            "Среднеквадратичная ошибка (MSE) для наивного прогноза: 27.085743750000002\n"
          ]
        }
      ],
      "source": [
        "y_pred = np.full_like(y, mean_sales)\n",
        "# посчитаем MSE\n",
        "mse = mse_error(y, y_pred)\n",
        "\n",
        "print(\"Наивный прогноз (среднее значение продаж):\", y_pred[0])\n",
        "print(\"Среднеквадратичная ошибка (MSE) для наивного прогноза:\", mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbybL2ola_ZM"
      },
      "source": [
        "**Создайте функцию *lin_pred*, которая может по матрице предикторов *X* и вектору весов линейной модели *w* получить вектор прогнозов** (0.5 балла)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Cyz-Luaa_ZO"
      },
      "outputs": [],
      "source": [
        "def lin_pred(X, w):\n",
        "    \"\"\"Функция получит предсказания по весам линейной модели\n",
        "\n",
        "    Args:\n",
        "        X (array): матрица предикторов\n",
        "        w (array): вектор весов линейной модели\n",
        "\n",
        "    Returns:\n",
        "        array: вектор прогнозов\n",
        "    \"\"\"\n",
        "    y_pred = X@w\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGZ16ylWXkIC",
        "outputId": "a8178665-a22b-4045-969e-c3f040dd65b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE: 2.784126314510936\n"
          ]
        }
      ],
      "source": [
        "# Мы произвели стандартизацию вектора Х, поэтому нам нужно обновить вектор весов w\n",
        "# Обновим веса w для отмасштабированных данных\n",
        "def lin_reg(X, y):\n",
        "    a = np.dot(X.T, X)\n",
        "    b = np.dot(X.T, y)\n",
        "    return np.linalg.solve(a, b)\n",
        "\n",
        "\n",
        "w = lin_reg(X, y).T\n",
        "\n",
        "y_pred = lin_pred(X, w)\n",
        "print(f'MSE: {mse_error(y, y_pred)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BU4adBrya_Zm"
      },
      "source": [
        "**Создайте функцию *stoch_grad_step* для реализации шага стохастического градиентного спуска. (1.5 балла)\n",
        "Функция должна принимать на вход следующие аргументы:**\n",
        "* матрицу *X*\n",
        "* вектора *y* и *w*\n",
        "* число *train_ind* - индекс объекта обучающей выборки (строки матрицы *X*), по которому считается изменение весов\n",
        "* число *$\\eta$* (eta) - шаг градиентного спуска\n",
        "\n",
        "Результатом будет вектор обновленных весов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyLY-P02DUK5"
      },
      "source": [
        "Шаг для стохастического градиентного спуска выглядит следующим образом:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORsAyIKNDUK5"
      },
      "source": [
        "$$\\Large w_j \\leftarrow w_j - \\frac{2\\eta}{\\ell} \\sum_{i=1}^\\ell{{x_{ij}((w_0 + w_1x_{i1} + w_2x_{i2} +  w_3x_{i3}) - y_i)}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQl2FrpuDUK6"
      },
      "source": [
        "Для того, чтобы написать функцию, нужно сделать следующее:\n",
        "    \n",
        "*  посчитать направление изменения: умножить объект обучающей выборки на 2 и на разницу между предсказанным значением и реальным, а потом поделить на количество элементов в выборке.\n",
        "* вернуть разницу между вектором весов и направлением изменения, умноженным на шаг градиентного спуска"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUhVQGsja_Zn"
      },
      "outputs": [],
      "source": [
        "def stoch_grad_step(X, y, w, index, eta=0.01):\n",
        "    \"\"\"summary\n",
        "\n",
        "    Args:\n",
        "        X (array): матрица предикторов\n",
        "        y (array): вектор ответов\n",
        "        w (array): вектор весов\n",
        "        train_ind (int): индекс объекта\n",
        "        eta (float): шаг градиентного спуска\n",
        "    Returns:\n",
        "        array: обновленный вектор весов\n",
        "    \"\"\"\n",
        "    x_sample = X[index]\n",
        "    y_sample = y[index]\n",
        "\n",
        "    y_pred = x_sample @ w\n",
        "\n",
        "    gradient = x_sample * (y_pred-y_sample) / len(X)\n",
        "    weights = w - 2 * eta * gradient\n",
        "\n",
        "    return weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdFvFST9XkII",
        "outputId": "b4863b28-2422-4f66-a489-b8e6dfa22db6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([14.02261487,  3.91934441,  2.79206845, -0.02267902])"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Опять же желательно проверять работоспособность функций\n",
        "stoch_grad_step(X, y, w, 11, 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXwIFd0Ma_Zx"
      },
      "source": [
        "**Создайте функцию *stochastic_gradient_descent*, для реализации стохастического градиентного спуска (2.5 балла)**\n",
        "\n",
        "**Функция принимает на вход следующие аргументы:**\n",
        "- Матрицу признаков X\n",
        "- Целевую переменнную\n",
        "- Изначальную точку (веса модели)\n",
        "- Параметр, определяющий темп обучения\n",
        "- Максимальное число итераций\n",
        "- Евклидово расстояние между векторами весов на соседних итерациях градиентного спуска,при котором алгоритм прекращает работу\n",
        "\n",
        "**На каждой итерации в вектор (список) должно записываться текущее значение среднеквадратичной ошибки. Функция должна возвращать вектор весов $w$, а также вектор (список) ошибок.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVeoNF1JDUK7"
      },
      "source": [
        "Алгоритм сследующий:\n",
        "    \n",
        "* Инициализируйте расстояние между векторами весов на соседних итерациях большим числом (можно бесконечностью)\n",
        "* Создайте пустой список для фиксации ошибок\n",
        "* Создайте счетчик итераций\n",
        "* Реализуйте оновной цикл обучения пока расстояние между векторами весов больше того, при котором надо прекратить работу (когда расстояния станут слишком маленькими - значит, мы застряли в одном месте) и количество итераций меньше максимально разрешенного: сгенерируйте случайный индекс, запишите текущую ошибку в вектор ошибок, запишите в переменную текущий шаг стохастического спуска с использованием функции, написанной ранее. Далее рассчитайте текущее расстояние между векторами весов и прибавьте к счетчику итераций 1.\n",
        "* Верните вектор весов и вектор ошибок"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CD_xcFNfa_Zy"
      },
      "outputs": [],
      "source": [
        "def stochastic_gradient_descent(X, y, w, eta=0.1, max_iter=1e+4, dist_min=1e-8):\n",
        "    \"\"\"Функция реализующая стохастический градиентный спуск\n",
        "\n",
        "    Args:\n",
        "        X (array): матрица предикторов\n",
        "        y (array): вектор ответов\n",
        "        w (array): вектор весов\n",
        "        eta (float): шаг градиентного спуска\n",
        "        max_iter (type): максимальное количество итерации\n",
        "        dist_min (type): минимальное расстояние между векторами весов\n",
        "    \"\"\"\n",
        "\n",
        "    distance = 1e+10 #расстояние между векторами\n",
        "    errors = [] #список для фиксации ошибок\n",
        "    iters = 0\n",
        "\n",
        "    w_value = []\n",
        "\n",
        "    while distance > dist_min and iters < max_iter:\n",
        "        random_ind = np.random.randint(X.shape[0])\n",
        "        y_pred = lin_pred(X, w)\n",
        "\n",
        "        errors.append(mse_error(y, y_pred))\n",
        "        w_new = stoch_grad_step(X, y, w, random_ind, eta)\n",
        "\n",
        "        distance = np.linalg.norm(w - w_new)\n",
        "        w_value.append(w)\n",
        "\n",
        "        w = w_new\n",
        "        iters += 1\n",
        "\n",
        "    return w, w_value, errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OqHO1Rta_Z7"
      },
      "source": [
        " **Запустите $10^5$ итераций стохастического градиентного спуска. Укажите вектор начальных весов, состоящий из нулей. Можете поэкспериментировать с параметром, отвечающим за темп обучения.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6fHHT6vDUK8"
      },
      "source": [
        "**Постройте график зависимости ошибки от номера итерации**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsSfHDzLDUK9",
        "outputId": "7583d648-270e-41ef-b379-0ee88e8b2b28"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x22232b635b0>]"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "w, w_values, errors = stochastic_gradient_descent(X, y, np.zeros(X.shape[1]), 2, 1e+5, dist_min=1e-3)\n",
        "plt.plot(errors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-MVOcJ6a_aY"
      },
      "source": [
        "**Выведите вектор весов, к которому сошелся метод.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPjVkXe4DUK9",
        "outputId": "835af777-6f45-4769-fcca-0e7338b5171b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Вектор весов, к которому сошелся метод: [14.38888619  3.45963264  3.17843814  0.0198313 ]\n"
          ]
        }
      ],
      "source": [
        "print(f'Вектор весов, к которому сошелся метод: {w}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qabzMc3Qa_a5"
      },
      "source": [
        "**Выведите среднеквадратичную ошибку на последней итерации.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tPWleMIa_a7",
        "outputId": "0b948c0d-adeb-43f4-96d2-135568dc41c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3.2706187661269586"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mse_error(X@w, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wT7oRjiNXkIK",
        "outputId": "e3f08028-fd7d-4df9-a296-d13442b144f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Минимальная целевая метрика достигнута на       224 итерации, и составила       2.807021724046557       \n",
            "Веса линейной регрессии при этом составили [14.07250021  3.96029435  2.91455238 -0.14065765]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\obond\\AppData\\Local\\Temp\\ipykernel_18272\\2623653776.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  w_optimal = model_data.iloc[model_data['MSE'].idxmin()][1]\n"
          ]
        }
      ],
      "source": [
        "# Для поиска итерации на которой достигнут минимум целевой метрики используем Pandas\n",
        "model_data = pd.DataFrame({'MSE': errors, 'w_value': w_values})\n",
        "w_optimal = model_data.iloc[model_data['MSE'].idxmin()][1]\n",
        "\n",
        "print(f'Минимальная целевая метрика достигнута на \\\n",
        "      {model_data[\"MSE\"].idxmin()} итерации, и составила \\\n",
        "      {errors[model_data[\"MSE\"].idxmin()]} \\\n",
        "      \\nВеса линейной регрессии при этом составили {w_optimal}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKGR1XM8XkIM"
      },
      "source": [
        "***\n",
        "если проще - градиентный спуск с учётом предыдущего шага\n",
        "***\n",
        "Формально можно записать, что для того, чтобы попасть в следующую точку $x_1$, необходимо перейти из начальной точки $x_0$ на антиградиент, домноженный на некоторый коэффициент, который называется **шагом градиентного спуска** или **темпом обучения** — о нём мы поговорим немного позже.\n",
        "\n",
        "$x_1 = x_0 - \\alpha \\nabla f (x_0)$\n",
        "\n",
        "$\\alpha$ - темп обучения\n",
        "\n",
        "$\\nabla f = \\left ( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right )$ - вектор градиента:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGvN0t7aXkIN"
      },
      "outputs": [],
      "source": [
        "# определим функцию\n",
        "def fun(x, y, a=1, b=1):\n",
        "    return a * (x**2) + b * (y**2)\n",
        "\n",
        "\n",
        "# определим градиент\n",
        "def grad(x, y, a=1, b=1):\n",
        "    return np.array([2 * a * x, 2 * b * y])\n",
        "\n",
        "\n",
        "# простой градиентный спуск\n",
        "def grad_descend(grad, step_size=0.2, num_steps=30):\n",
        "    lst = []\n",
        "    x = np.random.uniform(0, 3, size = 2)\n",
        "    lst.append(x)\n",
        "\n",
        "    for i in range(num_steps):\n",
        "        x = x - step_size * grad(lst[-1][0], lst[-1][1])\n",
        "        lst.append(x)\n",
        "\n",
        "    return np.array(lst)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjH3gSlpXkIN"
      },
      "source": [
        "Градиентный спуск с `momentum`. Формально его можно определить следующим образом:\n",
        "\n",
        "$x_{n+1}=x_n - \\alpha\\nabla f(x_n) + \\gamma(x_n - x_{n-1})$\n",
        "\n",
        "В формуле выше $\\gamma$ — это параметр, который показывает, насколько учитывается предыдущий шаг.\n",
        "\n",
        "Для примера найдём минимум функции $2x^2 - 4xy + y^4 + 2$ с помощью градиентного спуска."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brea_yI_XkIN",
        "outputId": "f2928a12-f0c6-4f68-e8b2-9882cecf5fb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter=1; x=(0.0800, 0.9200); f(x)=2.4348;           grad f(x)=(-3.3600, 2.7948)\n",
            "iter=2; x=(0.1472, 0.8641); f(x)=2.0921;           grad f(x)=(-2.8676, 1.9920)\n",
            "iter=3; x=(0.2046, 0.8243); f(x)=1.8709;           grad f(x)=(-2.4788, 1.4218)\n",
            "iter=4; x=(0.2541, 0.7958); f(x)=1.7213;           grad f(x)=(-2.1668, 0.9996)\n",
            "iter=5; x=(0.2975, 0.7758); f(x)=1.6161;           grad f(x)=(-1.9135, 0.6781)\n",
            "iter=6; x=(0.3357, 0.7623); f(x)=1.5394;           grad f(x)=(-1.7062, 0.4288)\n",
            "iter=7; x=(0.3699, 0.7537); f(x)=1.4812;           grad f(x)=(-1.5354, 0.2332)\n",
            "iter=8; x=(0.4006, 0.7490); f(x)=1.4355;           grad f(x)=(-1.3939, 0.0787)\n",
            "iter=9; x=(0.4284, 0.7475); f(x)=1.3983;           grad f(x)=(-1.2761, -0.0434)\n",
            "iter=10; x=(0.4540, 0.7483); f(x)=1.3669;           grad f(x)=(-1.1775, -0.1396)\n",
            "iter=11; x=(0.4775, 0.7511); f(x)=1.3397;           grad f(x)=(-1.0944, -0.2150)\n",
            "iter=12; x=(0.4994, 0.7554); f(x)=1.3154;           grad f(x)=(-1.0241, -0.2733)\n",
            "iter=13; x=(0.5199, 0.7609); f(x)=1.2935;           grad f(x)=(-0.9640, -0.3175)\n",
            "iter=14; x=(0.5392, 0.7672); f(x)=1.2732;           grad f(x)=(-0.9123, -0.3501)\n",
            "iter=15; x=(0.5574, 0.7742); f(x)=1.2545;           grad f(x)=(-0.8673, -0.3732)\n",
            "iter=16; x=(0.5748, 0.7817); f(x)=1.2369;           grad f(x)=(-0.8278, -0.3884)\n",
            "iter=17; x=(0.5913, 0.7895); f(x)=1.2205;           grad f(x)=(-0.7926, -0.3971)\n",
            "iter=18; x=(0.6072, 0.7974); f(x)=1.2050;           grad f(x)=(-0.7610, -0.4005)\n",
            "iter=19; x=(0.6224, 0.8054); f(x)=1.1904;           grad f(x)=(-0.7321, -0.3996)\n",
            "iter=20; x=(0.6370, 0.8134); f(x)=1.1767;           grad f(x)=(-0.7055, -0.3954)\n",
            "iter=21; x=(0.6511, 0.8213); f(x)=1.1638;           grad f(x)=(-0.6807, -0.3884)\n",
            "iter=22; x=(0.6648, 0.8291); f(x)=1.1517;           grad f(x)=(-0.6573, -0.3794)\n",
            "iter=23; x=(0.6779, 0.8367); f(x)=1.1404;           grad f(x)=(-0.6351, -0.3688)\n",
            "iter=24; x=(0.6906, 0.8441); f(x)=1.1298;           grad f(x)=(-0.6138, -0.3571)\n",
            "iter=25; x=(0.7029, 0.8512); f(x)=1.1199;           grad f(x)=(-0.5933, -0.3446)\n",
            "iter=26; x=(0.7147, 0.8581); f(x)=1.1106;           grad f(x)=(-0.5734, -0.3317)\n",
            "iter=27; x=(0.7262, 0.8647); f(x)=1.1020;           grad f(x)=(-0.5540, -0.3185)\n",
            "iter=28; x=(0.7373, 0.8711); f(x)=1.0940;           grad f(x)=(-0.5352, -0.3052)\n",
            "iter=29; x=(0.7480, 0.8772); f(x)=1.0865;           grad f(x)=(-0.5168, -0.2921)\n",
            "iter=30; x=(0.7583, 0.8830); f(x)=1.0796;           grad f(x)=(-0.4988, -0.2791)\n",
            "iter=31; x=(0.7683, 0.8886); f(x)=1.0732;           grad f(x)=(-0.4812, -0.2665)\n",
            "iter=32; x=(0.7779, 0.8939); f(x)=1.0673;           grad f(x)=(-0.4641, -0.2542)\n",
            "iter=33; x=(0.7872, 0.8990); f(x)=1.0618;           grad f(x)=(-0.4473, -0.2423)\n",
            "iter=34; x=(0.7962, 0.9039); f(x)=1.0567;           grad f(x)=(-0.4309, -0.2308)\n",
            "iter=35; x=(0.8048, 0.9085); f(x)=1.0520;           grad f(x)=(-0.4149, -0.2198)\n",
            "iter=36; x=(0.8131, 0.9129); f(x)=1.0477;           grad f(x)=(-0.3993, -0.2092)\n",
            "iter=37; x=(0.8211, 0.9171); f(x)=1.0437;           grad f(x)=(-0.3841, -0.1991)\n",
            "iter=38; x=(0.8287, 0.9211); f(x)=1.0400;           grad f(x)=(-0.3693, -0.1895)\n",
            "iter=39; x=(0.8361, 0.9248); f(x)=1.0367;           grad f(x)=(-0.3549, -0.1803)\n",
            "iter=40; x=(0.8432, 0.9285); f(x)=1.0336;           grad f(x)=(-0.3409, -0.1715)\n",
            "iter=41; x=(0.8500, 0.9319); f(x)=1.0307;           grad f(x)=(-0.3274, -0.1632)\n",
            "iter=42; x=(0.8566, 0.9351); f(x)=1.0281;           grad f(x)=(-0.3142, -0.1552)\n",
            "iter=43; x=(0.8629, 0.9382); f(x)=1.0257;           grad f(x)=(-0.3015, -0.1477)\n",
            "iter=44; x=(0.8689, 0.9412); f(x)=1.0235;           grad f(x)=(-0.2892, -0.1405)\n",
            "iter=45; x=(0.8747, 0.9440); f(x)=1.0215;           grad f(x)=(-0.2773, -0.1337)\n",
            "iter=46; x=(0.8802, 0.9467); f(x)=1.0196;           grad f(x)=(-0.2658, -0.1272)\n",
            "iter=47; x=(0.8855, 0.9492); f(x)=1.0179;           grad f(x)=(-0.2547, -0.1210)\n",
            "iter=48; x=(0.8906, 0.9517); f(x)=1.0163;           grad f(x)=(-0.2440, -0.1152)\n",
            "iter=49; x=(0.8955, 0.9540); f(x)=1.0149;           grad f(x)=(-0.2337, -0.1096)\n",
            "iter=50; x=(0.9002, 0.9561); f(x)=1.0136;           grad f(x)=(-0.2238, -0.1043)\n",
            "iter=51; x=(0.9047, 0.9582); f(x)=1.0124;           grad f(x)=(-0.2142, -0.0993)\n",
            "iter=52; x=(0.9090, 0.9602); f(x)=1.0113;           grad f(x)=(-0.2050, -0.0945)\n",
            "iter=53; x=(0.9131, 0.9621); f(x)=1.0103;           grad f(x)=(-0.1962, -0.0899)\n",
            "iter=54; x=(0.9170, 0.9639); f(x)=1.0094;           grad f(x)=(-0.1877, -0.0856)\n",
            "iter=55; x=(0.9207, 0.9656); f(x)=1.0086;           grad f(x)=(-0.1795, -0.0815)\n",
            "iter=56; x=(0.9243, 0.9672); f(x)=1.0078;           grad f(x)=(-0.1717, -0.0776)\n",
            "iter=57; x=(0.9278, 0.9688); f(x)=1.0071;           grad f(x)=(-0.1642, -0.0739)\n",
            "iter=58; x=(0.9310, 0.9703); f(x)=1.0065;           grad f(x)=(-0.1569, -0.0703)\n",
            "iter=59; x=(0.9342, 0.9717); f(x)=1.0059;           grad f(x)=(-0.1500, -0.0670)\n",
            "iter=60; x=(0.9372, 0.9730); f(x)=1.0054;           grad f(x)=(-0.1434, -0.0638)\n",
            "iter=61; x=(0.9401, 0.9743); f(x)=1.0049;           grad f(x)=(-0.1370, -0.0607)\n",
            "iter=62; x=(0.9428, 0.9755); f(x)=1.0045;           grad f(x)=(-0.1309, -0.0578)\n",
            "iter=63; x=(0.9454, 0.9767); f(x)=1.0041;           grad f(x)=(-0.1251, -0.0551)\n",
            "iter=64; x=(0.9479, 0.9778); f(x)=1.0037;           grad f(x)=(-0.1195, -0.0525)\n",
            "iter=65; x=(0.9503, 0.9788); f(x)=1.0034;           grad f(x)=(-0.1141, -0.0500)\n",
            "iter=66; x=(0.9526, 0.9798); f(x)=1.0031;           grad f(x)=(-0.1090, -0.0476)\n",
            "iter=67; x=(0.9548, 0.9808); f(x)=1.0028;           grad f(x)=(-0.1041, -0.0453)\n",
            "iter=68; x=(0.9568, 0.9817); f(x)=1.0026;           grad f(x)=(-0.0994, -0.0432)\n",
            "iter=69; x=(0.9588, 0.9825); f(x)=1.0023;           grad f(x)=(-0.0949, -0.0411)\n",
            "iter=70; x=(0.9607, 0.9834); f(x)=1.0021;           grad f(x)=(-0.0906, -0.0392)\n",
            "iter=71; x=(0.9625, 0.9842); f(x)=1.0019;           grad f(x)=(-0.0865, -0.0373)\n",
            "iter=72; x=(0.9643, 0.9849); f(x)=1.0017;           grad f(x)=(-0.0825, -0.0356)\n",
            "iter=73; x=(0.9659, 0.9856); f(x)=1.0016;           grad f(x)=(-0.0788, -0.0339)\n",
            "iter=74; x=(0.9675, 0.9863); f(x)=1.0014;           grad f(x)=(-0.0752, -0.0323)\n",
            "iter=75; x=(0.9690, 0.9869); f(x)=1.0013;           grad f(x)=(-0.0717, -0.0308)\n",
            "iter=76; x=(0.9704, 0.9875); f(x)=1.0012;           grad f(x)=(-0.0685, -0.0293)\n",
            "iter=77; x=(0.9718, 0.9881); f(x)=1.0011;           grad f(x)=(-0.0653, -0.0279)\n",
            "iter=78; x=(0.9731, 0.9887); f(x)=1.0010;           grad f(x)=(-0.0623, -0.0266)\n",
            "iter=79; x=(0.9744, 0.9892); f(x)=1.0009;           grad f(x)=(-0.0595, -0.0253)\n",
            "iter=80; x=(0.9755, 0.9897); f(x)=1.0008;           grad f(x)=(-0.0567, -0.0241)\n",
            "iter=81; x=(0.9767, 0.9902); f(x)=1.0007;           grad f(x)=(-0.0541, -0.0230)\n",
            "iter=82; x=(0.9778, 0.9907); f(x)=1.0007;           grad f(x)=(-0.0517, -0.0219)\n",
            "iter=83; x=(0.9788, 0.9911); f(x)=1.0006;           grad f(x)=(-0.0493, -0.0209)\n",
            "iter=84; x=(0.9798, 0.9915); f(x)=1.0006;           grad f(x)=(-0.0470, -0.0199)\n",
            "iter=85; x=(0.9807, 0.9919); f(x)=1.0005;           grad f(x)=(-0.0448, -0.0190)\n",
            "iter=86; x=(0.9816, 0.9923); f(x)=1.0005;           grad f(x)=(-0.0428, -0.0181)\n",
            "iter=87; x=(0.9825, 0.9927); f(x)=1.0004;           grad f(x)=(-0.0408, -0.0172)\n",
            "iter=88; x=(0.9833, 0.9930); f(x)=1.0004;           grad f(x)=(-0.0389, -0.0164)\n",
            "iter=89; x=(0.9841, 0.9933); f(x)=1.0003;           grad f(x)=(-0.0371, -0.0156)\n",
            "iter=90; x=(0.9848, 0.9937); f(x)=1.0003;           grad f(x)=(-0.0354, -0.0149)\n",
            "iter=91; x=(0.9855, 0.9940); f(x)=1.0003;           grad f(x)=(-0.0337, -0.0142)\n",
            "iter=92; x=(0.9862, 0.9942); f(x)=1.0003;           grad f(x)=(-0.0322, -0.0135)\n",
            "iter=93; x=(0.9868, 0.9945); f(x)=1.0002;           grad f(x)=(-0.0307, -0.0129)\n",
            "iter=94; x=(0.9874, 0.9948); f(x)=1.0002;           grad f(x)=(-0.0293, -0.0123)\n",
            "iter=95; x=(0.9880, 0.9950); f(x)=1.0002;           grad f(x)=(-0.0279, -0.0117)\n",
            "iter=96; x=(0.9886, 0.9952); f(x)=1.0002;           grad f(x)=(-0.0266, -0.0112)\n",
            "iter=97; x=(0.9891, 0.9955); f(x)=1.0002;           grad f(x)=(-0.0254, -0.0106)\n",
            "iter=98; x=(0.9896, 0.9957); f(x)=1.0001;           grad f(x)=(-0.0242, -0.0101)\n",
            "iter=99; x=(0.9901, 0.9959); f(x)=1.0001;           grad f(x)=(-0.0231, -0.0097)\n",
            "iter=100; x=(0.9906, 0.9961); f(x)=1.0001;           grad f(x)=(-0.0220, -0.0092)\n",
            "iter=101; x=(0.9910, 0.9963); f(x)=1.0001;           grad f(x)=(-0.0210, -0.0088)\n",
            "iter=102; x=(0.9914, 0.9964); f(x)=1.0001;           grad f(x)=(-0.0200, -0.0084)\n",
            "iter=103; x=(0.9918, 0.9966); f(x)=1.0001;           grad f(x)=(-0.0191, -0.0080)\n",
            "iter=104; x=(0.9922, 0.9968); f(x)=1.0001;           grad f(x)=(-0.0182, -0.0076)\n",
            "iter=105; x=(0.9926, 0.9969); f(x)=1.0001;           grad f(x)=(-0.0173, -0.0072)\n",
            "iter=106; x=(0.9929, 0.9971); f(x)=1.0001;           grad f(x)=(-0.0165, -0.0069)\n",
            "iter=107; x=(0.9933, 0.9972); f(x)=1.0001;           grad f(x)=(-0.0158, -0.0066)\n",
            "iter=108; x=(0.9936, 0.9973); f(x)=1.0001;           grad f(x)=(-0.0150, -0.0063)\n",
            "iter=109; x=(0.9939, 0.9975); f(x)=1.0001;           grad f(x)=(-0.0143, -0.0060)\n",
            "iter=110; x=(0.9942, 0.9976); f(x)=1.0000;           grad f(x)=(-0.0137, -0.0057)\n",
            "iter=111; x=(0.9944, 0.9977); f(x)=1.0000;           grad f(x)=(-0.0130, -0.0054)\n",
            "iter=112; x=(0.9947, 0.9978); f(x)=1.0000;           grad f(x)=(-0.0124, -0.0052)\n",
            "iter=113; x=(0.9949, 0.9979); f(x)=1.0000;           grad f(x)=(-0.0118, -0.0049)\n",
            "iter=114; x=(0.9952, 0.9980); f(x)=1.0000;           grad f(x)=(-0.0113, -0.0047)\n",
            "iter=115; x=(0.9954, 0.9981); f(x)=1.0000;           grad f(x)=(-0.0108, -0.0045)\n",
            "iter=116; x=(0.9956, 0.9982); f(x)=1.0000;           grad f(x)=(-0.0102, -0.0043)\n",
            "iter=117; x=(0.9958, 0.9983); f(x)=1.0000;           grad f(x)=(-0.0098, -0.0041)\n",
            "iter=118; x=(0.9960, 0.9983); f(x)=1.0000;           grad f(x)=(-0.0093, -0.0039)\n",
            "iter=119; x=(0.9962, 0.9984); f(x)=1.0000;           grad f(x)=(-0.0089, -0.0037)\n",
            "iter=120; x=(0.9964, 0.9985); f(x)=1.0000;           grad f(x)=(-0.0085, -0.0035)\n",
            "iter=121; x=(0.9966, 0.9986); f(x)=1.0000;           grad f(x)=(-0.0081, -0.0034)\n",
            "iter=122; x=(0.9967, 0.9986); f(x)=1.0000;           grad f(x)=(-0.0077, -0.0032)\n",
            "iter=123; x=(0.9969, 0.9987); f(x)=1.0000;           grad f(x)=(-0.0073, -0.0030)\n",
            "iter=124; x=(0.9970, 0.9988); f(x)=1.0000;           grad f(x)=(-0.0070, -0.0029)\n",
            "iter=125; x=(0.9972, 0.9988); f(x)=1.0000;           grad f(x)=(-0.0067, -0.0028)\n",
            "iter=126; x=(0.9973, 0.9989); f(x)=1.0000;           grad f(x)=(-0.0064, -0.0026)\n",
            "iter=127; x=(0.9974, 0.9989); f(x)=1.0000;           grad f(x)=(-0.0061, -0.0025)\n",
            "iter=128; x=(0.9975, 0.9990); f(x)=1.0000;           grad f(x)=(-0.0058, -0.0024)\n",
            "iter=129; x=(0.9977, 0.9990); f(x)=1.0000;           grad f(x)=(-0.0055, -0.0023)\n",
            "iter=130; x=(0.9978, 0.9991); f(x)=1.0000;           grad f(x)=(-0.0052, -0.0022)\n",
            "iter=131; x=(0.9979, 0.9991); f(x)=1.0000;           grad f(x)=(-0.0050, -0.0021)\n",
            "iter=132; x=(0.9980, 0.9992); f(x)=1.0000;           grad f(x)=(-0.0048, -0.0020)\n",
            "iter=133; x=(0.9981, 0.9992); f(x)=1.0000;           grad f(x)=(-0.0045, -0.0019)\n",
            "iter=134; x=(0.9982, 0.9992); f(x)=1.0000;           grad f(x)=(-0.0043, -0.0018)\n",
            "iter=135; x=(0.9982, 0.9993); f(x)=1.0000;           grad f(x)=(-0.0041, -0.0017)\n",
            "iter=136; x=(0.9983, 0.9993); f(x)=1.0000;           grad f(x)=(-0.0039, -0.0016)\n",
            "iter=137; x=(0.9984, 0.9993); f(x)=1.0000;           grad f(x)=(-0.0037, -0.0016)\n",
            "iter=138; x=(0.9985, 0.9994); f(x)=1.0000;           grad f(x)=(-0.0036, -0.0015)\n",
            "iter=139; x=(0.9985, 0.9994); f(x)=1.0000;           grad f(x)=(-0.0034, -0.0014)\n",
            "iter=140; x=(0.9986, 0.9994); f(x)=1.0000;           grad f(x)=(-0.0032, -0.0013)\n",
            "iter=141; x=(0.9987, 0.9995); f(x)=1.0000;           grad f(x)=(-0.0031, -0.0013)\n",
            "iter=142; x=(0.9987, 0.9995); f(x)=1.0000;           grad f(x)=(-0.0029, -0.0012)\n",
            "iter=143; x=(0.9988, 0.9995); f(x)=1.0000;           grad f(x)=(-0.0028, -0.0012)\n",
            "iter=144; x=(0.9989, 0.9995); f(x)=1.0000;           grad f(x)=(-0.0027, -0.0011)\n",
            "iter=145; x=(0.9989, 0.9995); f(x)=1.0000;           grad f(x)=(-0.0026, -0.0011)\n",
            "iter=146; x=(0.9990, 0.9996); f(x)=1.0000;           grad f(x)=(-0.0024, -0.0010)\n",
            "iter=147; x=(0.9990, 0.9996); f(x)=1.0000;           grad f(x)=(-0.0023, -0.0010)\n",
            "iter=148; x=(0.9991, 0.9996); f(x)=1.0000;           grad f(x)=(-0.0022, -0.0009)\n",
            "iter=149; x=(0.9991, 0.9996); f(x)=1.0000;           grad f(x)=(-0.0021, -0.0009)\n",
            "iter=150; x=(0.9991, 0.9996); f(x)=1.0000;           grad f(x)=(-0.0020, -0.0008)\n",
            "iter=151; x=(0.9992, 0.9997); f(x)=1.0000;           grad f(x)=(-0.0019, -0.0008)\n",
            "iter=152; x=(0.9992, 0.9997); f(x)=1.0000;           grad f(x)=(-0.0018, -0.0008)\n",
            "iter=153; x=(0.9993, 0.9997); f(x)=1.0000;           grad f(x)=(-0.0017, -0.0007)\n",
            "iter=154; x=(0.9993, 0.9997); f(x)=1.0000;           grad f(x)=(-0.0017, -0.0007)\n",
            "iter=155; x=(0.9993, 0.9997); f(x)=1.0000;           grad f(x)=(-0.0016, -0.0007)\n",
            "iter=156; x=(0.9994, 0.9997); f(x)=1.0000;           grad f(x)=(-0.0015, -0.0006)\n",
            "iter=157; x=(0.9994, 0.9997); f(x)=1.0000;           grad f(x)=(-0.0014, -0.0006)\n",
            "iter=158; x=(0.9994, 0.9998); f(x)=1.0000;           grad f(x)=(-0.0014, -0.0006)\n",
            "iter=159; x=(0.9994, 0.9998); f(x)=1.0000;           grad f(x)=(-0.0013, -0.0005)\n",
            "iter=160; x=(0.9995, 0.9998); f(x)=1.0000;           grad f(x)=(-0.0012, -0.0005)\n",
            "iter=161; x=(0.9995, 0.9998); f(x)=1.0000;           grad f(x)=(-0.0012, -0.0005)\n",
            "iter=162; x=(0.9995, 0.9998); f(x)=1.0000;           grad f(x)=(-0.0011, -0.0005)\n",
            "iter=163; x=(0.9995, 0.9998); f(x)=1.0000;           grad f(x)=(-0.0011, -0.0004)\n",
            "iter=164; x=(0.9996, 0.9998); f(x)=1.0000;           grad f(x)=(-0.0010, -0.0004)\n",
            "iter=165; x=(0.9996, 0.9998); f(x)=1.0000;           grad f(x)=(-0.0010, -0.0004)\n",
            "iter=166; x=(0.9996, 0.9998); f(x)=1.0000;           grad f(x)=(-0.0009, -0.0004)\n",
            "iter=167; x=(0.9996, 0.9998); f(x)=1.0000;           grad f(x)=(-0.0009, -0.0004)\n",
            "iter=168; x=(0.9996, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0008, -0.0004)\n",
            "iter=169; x=(0.9997, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0008, -0.0003)\n",
            "iter=170; x=(0.9997, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0008, -0.0003)\n",
            "iter=171; x=(0.9997, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0007, -0.0003)\n",
            "iter=172; x=(0.9997, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0007, -0.0003)\n",
            "iter=173; x=(0.9997, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0007, -0.0003)\n",
            "iter=174; x=(0.9997, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0006, -0.0003)\n",
            "iter=175; x=(0.9997, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0006, -0.0003)\n",
            "iter=176; x=(0.9998, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0006, -0.0002)\n",
            "iter=177; x=(0.9998, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0006, -0.0002)\n",
            "iter=178; x=(0.9998, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0005, -0.0002)\n",
            "iter=179; x=(0.9998, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0005, -0.0002)\n",
            "iter=180; x=(0.9998, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0005, -0.0002)\n",
            "iter=181; x=(0.9998, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0005, -0.0002)\n",
            "iter=182; x=(0.9998, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0004, -0.0002)\n",
            "iter=183; x=(0.9998, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0004, -0.0002)\n",
            "iter=184; x=(0.9998, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0004, -0.0002)\n",
            "iter=185; x=(0.9998, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0004, -0.0002)\n",
            "iter=186; x=(0.9998, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0004, -0.0001)\n",
            "iter=187; x=(0.9999, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0003, -0.0001)\n",
            "iter=188; x=(0.9999, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0003, -0.0001)\n",
            "iter=189; x=(0.9999, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0003, -0.0001)\n",
            "iter=190; x=(0.9999, 0.9999); f(x)=1.0000;           grad f(x)=(-0.0003, -0.0001)\n",
            "iter=191; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0003, -0.0001)\n",
            "iter=192; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0003, -0.0001)\n",
            "iter=193; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0003, -0.0001)\n",
            "iter=194; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0002, -0.0001)\n",
            "iter=195; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0002, -0.0001)\n",
            "iter=196; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0002, -0.0001)\n",
            "iter=197; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0002, -0.0001)\n",
            "iter=198; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0002, -0.0001)\n",
            "iter=199; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0002, -0.0001)\n",
            "iter=200; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0002, -0.0001)\n",
            "iter=201; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0002, -0.0001)\n",
            "iter=202; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0002, -0.0001)\n",
            "iter=203; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0002, -0.0001)\n",
            "iter=204; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0002, -0.0001)\n",
            "iter=205; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0001)\n",
            "iter=206; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0001)\n",
            "iter=207; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0001)\n",
            "iter=208; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0001)\n",
            "iter=209; x=(0.9999, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0000)\n",
            "iter=210; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0000)\n",
            "iter=211; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0000)\n",
            "iter=212; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0000)\n",
            "iter=213; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0000)\n",
            "iter=214; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0000)\n",
            "iter=215; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0000)\n",
            "iter=216; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0000)\n",
            "iter=217; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0000)\n",
            "iter=218; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0000)\n",
            "iter=219; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0000)\n",
            "iter=220; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0000)\n",
            "iter=221; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0000)\n",
            "iter=222; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0000)\n",
            "iter=223; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0000)\n",
            "iter=224; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0000)\n",
            "iter=225; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0000)\n",
            "iter=226; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0001, -0.0000)\n",
            "iter=227; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=228; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=229; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=230; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=231; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=232; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=233; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=234; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=235; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=236; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=237; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=238; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=239; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=240; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=241; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=242; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=243; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=244; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=245; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=246; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=247; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=248; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=249; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=250; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=251; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=252; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=253; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=254; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=255; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=256; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=257; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=258; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=259; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=260; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=261; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=262; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=263; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=264; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=265; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=266; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=267; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=268; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=269; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=270; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=271; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=272; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=273; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=274; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=275; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=276; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=277; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=278; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=279; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=280; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=281; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=282; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=283; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=284; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=285; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=286; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=287; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=288; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=289; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=290; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=291; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=292; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=293; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=294; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=295; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=296; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=297; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=298; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=299; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=300; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n",
            "iter=301; x=(1.0000, 1.0000); f(x)=1.0000;           grad f(x)=(-0.0000, -0.0000)\n"
          ]
        }
      ],
      "source": [
        "def f(x, y): # сама функция далее не вызывается\n",
        "    return 2*x**2-4*x*y + y**4 + 2 # но объявим её\n",
        "\n",
        "\n",
        "def grad(x, y): # вычислятель градиента\n",
        "    dx = 4*x-4*y # частная производная по х\n",
        "    dy = 4*y**3-4*x # частная производная по у\n",
        "    return (dx, dy)\n",
        "\n",
        "\n",
        "x0 = (0, 1) # начальная точка\n",
        "gamma = 0.02 # темп обучения\n",
        "x_cur = x0 # текущая точка (на 1ой итерации совпадает с начальной)\n",
        "\n",
        "vals = []\n",
        "coords = []\n",
        "i = 0\n",
        "\n",
        "while True: # запускаем бесконечный цикл\n",
        "    # метод градиентного спуска посчитанный покординатно\n",
        "    x_new = (x_cur[0] - gamma * grad(*x_cur)[0],\n",
        "            x_cur[1] - gamma * grad(*x_cur)[1])\n",
        "\n",
        "    if i > 300: # зададим конечное количество итераций\n",
        "        break\n",
        "\n",
        "    x_cur = x_new # перезапишем текущую точку\n",
        "    vals.append(f(*x_cur))\n",
        "    coords.append(x_cur)\n",
        "    i += 1 # добавим счётчик итераций\n",
        "\n",
        "    # выведем интересующие нас значения\n",
        "    print(f\"iter={i}; x=({x_cur[0]:.4f}, {x_cur[1]:.4f});\"\\\n",
        "          f\" f(x)={f(*x_cur):.4f}; \\\n",
        "          grad f(x)=({grad(*x_cur)[0]:.4f}, {grad(*x_cur)[1]:.4f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcWZfyYeXkIO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}